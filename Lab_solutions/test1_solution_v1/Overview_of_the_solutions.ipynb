{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this test, I used gradient boosting treees (GBT), LDA and SGDC methods on the server. My X is those derived parameters and Y is the label (signal [1] or background [2]). One thing, I do not understand is the weight, so it has not been taken into account.\n",
    "\n",
    "The outputted models are gbt.pkl, lda.pkl and sgdc.pkl, for GBT, LDA and SGDC,  respectively.\n",
    "\n",
    "The best outputted AMS are ~2.6, ~1.5, ~0.8 for GBT, LDA and SGDC, respectively. Apparently, GBT method outperformed another two methods. \n",
    "\n",
    "There is also one other code, which uses the xgboost method. The original code is found online, which is one of the winning solutions for the same problem. The original code can be found here: https://github.com/phunterlau/kaggle_higgs\n",
    "\n",
    "Since there is no xgboost module on the server, I have installed it on my own computer. But I did not exactly follow the original code. For example, in that code, it has self-deduced parameters rather than the derived parameter in the dataset. I made it run on my computer, and it ends up with highest AMS at ~3.6. It seems this method is better than those 3 I proposed, therefore, there might be some misunderstandings of the problem, probably due to the weight stuff or that it might be necessary to add some other parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
